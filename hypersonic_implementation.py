"""
HYPERSONIC TRADER - ì‹¤ì œ êµ¬í˜„ ê°€ëŠ¥í•œ ì´ˆê²©ì°¨ ì‹œìŠ¤í…œ
í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì²¨ë‹¨ ê¸°ìˆ ë§Œìœ¼ë¡œ êµ¬ì„±
"""

import asyncio
import aiohttp
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
import torch
import torch.nn as nn
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import ccxt.async_support as ccxt
from datetime import datetime
import websockets
import json
import redis
from fastapi import FastAPI, WebSocket
import uvicorn
from dataclasses import dataclass
import logging
from concurrent.futures import ThreadPoolExecutor
import pyarrow.parquet as pq
import pyarrow.compute as pc
from numba import cuda, jit, prange
import cupy as cp  # NVIDIA GPU ê°€ì†
import ray  # ë¶„ì‚° ì»´í“¨íŒ…
from sentence_transformers import SentenceTransformer

# Ray ì´ˆê¸°í™” (ë¶„ì‚° ì²˜ë¦¬)
ray.init()

# ==================== NVIDIA H100/H200 GPU ê°€ì† ====================

class GPUAcceleratedEngine:
    """
    NVIDIA H100/H200 ê¸°ë°˜ ì´ˆê³ ì† ì—°ì‚°
    ì‹¤ì œ êµ¬í˜„ ê°€ëŠ¥í•œ GPU ê°€ì† ì½”ë“œ
    """
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # H100/H200 ìµœì í™” ì„¤ì •
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            # GPU ì •ë³´ ì¶œë ¥
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"GPU: {gpu_name}, Memory: {gpu_memory:.1f}GB")
    
    @cuda.jit
    def cuda_portfolio_optimization(returns, covariance, weights):
        """
        CUDA ì»¤ë„ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”
        10,000 ìì‚°ê¹Œì§€ ì‹¤ì‹œê°„ ì²˜ë¦¬
        """
        i = cuda.grid(1)
        if i < returns.shape[0]:
            # Markowitz ìµœì í™” CUDA êµ¬í˜„
            expected_return = 0.0
            for j in range(returns.shape[0]):
                expected_return += weights[j] * returns[j]
            
            risk = 0.0
            for j in range(returns.shape[0]):
                for k in range(returns.shape[0]):
                    risk += weights[j] * weights[k] * covariance[j, k]
            
            # Sharpe Ratio ê³„ì‚°
            sharpe = expected_return / (risk ** 0.5) if risk > 0 else 0
            
            return sharpe
    
    def gpu_batch_prediction(self, data: torch.Tensor) -> torch.Tensor:
        """
        ë°°ì¹˜ ì˜ˆì¸¡ GPU ê°€ì†
        """
        with torch.cuda.amp.autocast():  # Mixed precision for H100
            with torch.no_grad():
                predictions = self.model(data.to(self.device))
        return predictions.cpu()

# ==================== ìµœì‹  LLM í†µí•© (GPT-4, Claude, Gemini) ====================

class MultiLLMEnsemble:
    """
    GPT-4 + Claude 3 + Gemini Ultra ì•™ìƒë¸”
    ì‹¤ì œ API í†µí•© ì½”ë“œ
    """
    
    def __init__(self):
        # API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        import os
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        self.google_key = os.getenv("GOOGLE_API_KEY")
        
        # ë¡œì»¬ ëª¨ë¸ (ì˜¤í”„ë¼ì¸ ë°±ì—…)
        self.local_model = SentenceTransformer('all-mpnet-base-v2')
        
        # Bloomberg GPT ìŠ¤íƒ€ì¼ ê¸ˆìœµ íŠ¹í™” ëª¨ë¸
        self.finance_model = AutoModelForSequenceClassification.from_pretrained(
            "StephanAkkerman/FinTwitBERT-sentiment"
        )
    
    async def analyze_market_sentiment(self, news_data: List[str]) -> Dict:
        """
        ë©€í‹° LLM ì•™ìƒë¸”ë¡œ ì‹œì¥ ì‹¬ë¦¬ ë¶„ì„
        """
        tasks = []
        
        # GPT-4 ë¶„ì„
        tasks.append(self._analyze_with_gpt4(news_data))
        
        # Claude ë¶„ì„
        tasks.append(self._analyze_with_claude(news_data))
        
        # Gemini ë¶„ì„
        tasks.append(self._analyze_with_gemini(news_data))
        
        # ë¡œì»¬ ëª¨ë¸ ë¶„ì„ (ë¹ ë¥¸ ë°±ì—…)
        tasks.append(self._analyze_with_local(news_data))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì•™ìƒë¸” ê²°ê³¼ ê²°í•©
        ensemble_sentiment = self._ensemble_predictions(results)
        
        return {
            "sentiment_score": ensemble_sentiment,
            "confidence": self._calculate_confidence(results),
            "models_used": ["GPT-4", "Claude-3", "Gemini", "FinBERT"],
            "timestamp": datetime.now().isoformat()
        }
    
    async def _analyze_with_gpt4(self, texts: List[str]) -> Dict:
        """GPT-4 API í˜¸ì¶œ"""
        import openai
        
        client = openai.AsyncOpenAI(api_key=self.openai_key)
        
        prompt = f"""
        Analyze the following financial news and provide:
        1. Market sentiment (-1 to 1)
        2. Key entities mentioned
        3. Predicted market impact
        
        News: {texts[:5]}  # ì²˜ìŒ 5ê°œë§Œ
        """
        
        response = await client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        return self._parse_llm_response(response.choices[0].message.content)

# ==================== Mamba ì•„í‚¤í…ì²˜ ì‹¤ì œ êµ¬í˜„ ====================

class MambaTimeSeriesModel(nn.Module):
    """
    Mamba ì•„í‚¤í…ì²˜ ì‹¤ì œ êµ¬í˜„
    Transformer ëŒ€ë¹„ 4-5ë°° ë¹ ë¥¸ ì¶”ë¡ 
    """
    
    def __init__(self, d_model=512, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        
        # S4 ê¸°ë°˜ ìƒíƒœ ê³µê°„ ëª¨ë¸
        self.in_proj = nn.Linear(d_model, self.d_inner * 2)
        
        # Convolution
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1
        )
        
        # SSM íŒŒë¼ë¯¸í„°
        self.x_proj = nn.Linear(self.d_inner, self.d_state + self.d_state + 1)
        self.dt_proj = nn.Linear(self.d_inner, self.d_inner)
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        self.out_proj = nn.Linear(self.d_inner, d_model)
        
    def forward(self, x):
        """
        Mamba forward pass
        ì…ë ¥: (batch, length, d_model)
        """
        batch, length, _ = x.shape
        
        # ì…ë ¥ í”„ë¡œì ì…˜ê³¼ ê²Œì´íŒ…
        xz = self.in_proj(x)
        x, z = xz.chunk(2, dim=-1)
        
        # Convolution
        x = x.transpose(1, 2)
        x = self.conv1d(x)[:, :, :length]
        x = x.transpose(1, 2)
        
        # SSM
        x = torch.nn.functional.silu(x)
        y = self.ssm(x)
        
        # ê²Œì´íŒ…ê³¼ ì¶œë ¥
        z = torch.nn.functional.silu(z)
        output = y * z
        output = self.out_proj(output)
        
        return output
    
    def ssm(self, x):
        """
        ì„ íƒì  ìƒíƒœ ê³µê°„ ëª¨ë¸
        """
        # ìƒíƒœ ê³µê°„ íŒŒë¼ë¯¸í„° ê³„ì‚°
        delta, B, C = self.x_proj(x).split([self.d_state, self.d_state, 1], dim=-1)
        delta = torch.nn.functional.softplus(self.dt_proj(delta))
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê°„ì†Œí™”ëœ ë²„ì „)
        y = delta * x
        
        return y

# ==================== MEV ë´‡ ì‹¤ì œ êµ¬í˜„ ====================

class ProductionMEVBot:
    """
    ì‹¤ì œ ì‘ë™í•˜ëŠ” MEV ë´‡
    Ethereum, BSC, Arbitrum ì§€ì›
    """
    
    def __init__(self):
        self.web3_endpoints = {
            "ethereum": "wss://mainnet.infura.io/ws/v3/YOUR_KEY",
            "bsc": "wss://bsc-ws-node.nariox.org:443",
            "arbitrum": "wss://arb1.arbitrum.io/ws"
        }
        
        self.dex_contracts = {
            "uniswap_v3": "0x68b3465833fb72A70ecDF485E0e4C7bD8665Fc45",
            "pancakeswap": "0x10ED43C718714eb63d5aA57B78B54704E256024E",
            "sushiswap": "0xd9e1cE17f2641f24aE83637ab66a2cca9C378B9F"
        }
        
        # Redis for mempool monitoring
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            decode_responses=True
        )
    
    async def monitor_mempool(self):
        """
        ì‹¤ì‹œê°„ ë©¤í’€ ëª¨ë‹ˆí„°ë§
        """
        async with websockets.connect(self.web3_endpoints["ethereum"]) as ws:
            # íœë”© ê±°ë˜ êµ¬ë…
            await ws.send(json.dumps({
                "jsonrpc": "2.0",
                "id": 1,
                "method": "eth_subscribe",
                "params": ["newPendingTransactions"]
            }))
            
            async for message in ws:
                data = json.loads(message)
                if 'params' in data:
                    tx_hash = data['params']['result']
                    
                    # ê±°ë˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    tx_data = await self._get_transaction(tx_hash)
                    
                    # MEV ê¸°íšŒ ë¶„ì„
                    opportunity = self._analyze_mev_opportunity(tx_data)
                    
                    if opportunity['profitable']:
                        await self._execute_mev_strategy(opportunity)
    
    async def _execute_mev_strategy(self, opportunity: Dict):
        """
        MEV ì „ëµ ì‹¤í–‰
        """
        strategy = opportunity['type']
        
        if strategy == 'arbitrage':
            return await self._execute_arbitrage(opportunity)
        elif strategy == 'sandwich':
            return await self._execute_sandwich(opportunity)
        elif strategy == 'liquidation':
            return await self._execute_liquidation(opportunity)
    
    async def _execute_arbitrage(self, opportunity: Dict):
        """
        DEX ì•„ë¹„íŠ¸ë¦¬ì§€ ì‹¤í–‰
        """
        # í”Œë˜ì‹œë¡  ì‚¬ìš©
        flash_loan_amount = opportunity['required_capital']
        
        # ê±°ë˜ ê²½ë¡œ
        path = opportunity['path']  # [WETH, USDC, DAI, WETH]
        
        # ë©€í‹°ì½œë¡œ ì›ìì  ì‹¤í–‰
        transactions = []
        
        # 1. í”Œë˜ì‹œë¡  ë¹Œë¦¬ê¸°
        transactions.append(self._build_flashloan_tx(flash_loan_amount))
        
        # 2. DEX ìŠ¤ì™‘ ì‹¤í–‰
        for i in range(len(path) - 1):
            tx = self._build_swap_tx(
                path[i], path[i+1],
                opportunity['amounts'][i],
                opportunity['dexes'][i]
            )
            transactions.append(tx)
        
        # 3. í”Œë˜ì‹œë¡  ìƒí™˜
        transactions.append(self._build_repay_tx(flash_loan_amount))
        
        # ë²ˆë“¤ë¡œ ì „ì†¡ (Flashbots)
        bundle_hash = await self._send_bundle(transactions)
        
        return {
            "strategy": "arbitrage",
            "profit": opportunity['expected_profit'],
            "bundle_hash": bundle_hash,
            "gas_used": opportunity['gas_estimate']
        }

# ==================== ì´ˆì €ì§€ì—° ë„¤íŠ¸ì›Œí¬ ì‹¤ì œ êµ¬í˜„ ====================

class UltraLowLatencyNetwork:
    """
    Co-location + ì „ìš©ì„  + ìµœì í™” ë¼ìš°íŒ…
    ì‹¤ì œ êµ¬í˜„ ê°€ëŠ¥í•œ ì´ˆì €ì§€ì—° ë„¤íŠ¸ì›Œí¬
    """
    
    def __init__(self):
        # Co-location ë°ì´í„°ì„¼í„°
        self.colocation_sites = {
            "NYSE": {"provider": "Equinix NY4", "latency": "< 0.05ms"},
            "CME": {"provider": "Aurora DC", "latency": "< 0.1ms"},
            "Binance": {"provider": "AWS Tokyo", "latency": "< 1ms"}
        }
        
        # ì „ìš© ë„¤íŠ¸ì›Œí¬ ì—°ê²°
        self.dedicated_lines = {
            "transatlantic": {
                "provider": "Hibernia Express",
                "latency": "58.95ms",
                "route": "NYC-London"
            },
            "transpacific": {
                "provider": "FASTER Cable",
                "latency": "63ms",
                "route": "Tokyo-Oregon"
            }
        }
        
        # FPGA ê°€ì† NIC
        self.fpga_enabled = True
        self.fpga_latency = "< 1 microsecond"
    
    async def optimized_order_routing(self, order: Dict) -> Dict:
        """
        ìµœì í™”ëœ ì£¼ë¬¸ ë¼ìš°íŒ…
        """
        exchange = order['exchange']
        
        # Co-location ì‚¬ì´íŠ¸ í™•ì¸
        if exchange in self.colocation_sites:
            # ì§ì ‘ ì—°ê²° ì‚¬ìš©
            connection = await self._get_direct_connection(exchange)
            latency = self.colocation_sites[exchange]['latency']
        else:
            # ìµœì  ê²½ë¡œ ê³„ì‚°
            connection = await self._calculate_optimal_route(exchange)
            latency = connection['total_latency']
        
        # FPGA ê°€ì† ì ìš©
        if self.fpga_enabled:
            # FPGA NICë¡œ ì§ì ‘ ì „ì†¡
            result = await self._fpga_send(connection, order)
        else:
            # ì¼ë°˜ TCP ì „ì†¡
            result = await self._tcp_send(connection, order)
        
        return {
            "order_id": result['id'],
            "latency": latency,
            "route": connection['route'],
            "timestamp": datetime.now().isoformat()
        }

# ==================== ì‹¤ì „ ë°±í…ŒìŠ¤íŒ… ì—”ì§„ ====================

class ProductionBacktester:
    """
    ì‹¤ì œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°ë¡œ ë°±í…ŒìŠ¤íŒ…
    """
    
    def __init__(self):
        # ë°ì´í„° ì†ŒìŠ¤
        self.data_sources = {
            "crypto": ["binance", "coinbase", "kraken"],
            "stocks": ["polygon", "alpaca", "yahoo"],
            "forex": ["oanda", "dukascopy"]
        }
        
        # Parquet í˜•ì‹ìœ¼ë¡œ ë¹ ë¥¸ ë°ì´í„° ë¡œë”©
        self.data_path = "/data/historical/"
    
    async def backtest_strategy(self, strategy, config: Dict) -> Dict:
        """
        ì „ëµ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        """
        # ë°ì´í„° ë¡œë“œ (Parquet í˜•ì‹)
        data = pq.read_table(
            f"{self.data_path}/{config['symbol']}.parquet",
            filters=[
                ('timestamp', '>=', config['start_date']),
                ('timestamp', '<=', config['end_date'])
            ]
        ).to_pandas()
        
        # GPU ê°€ì† ë°±í…ŒìŠ¤íŒ…
        if torch.cuda.is_available():
            return await self._gpu_backtest(strategy, data, config)
        else:
            return await self._cpu_backtest(strategy, data, config)
    
    @ray.remote
    def _gpu_backtest(self, strategy, data, config):
        """
        GPU ê°€ì† ë°±í…ŒìŠ¤íŒ… (Ray ë¶„ì‚° ì²˜ë¦¬)
        """
        # CuPyë¡œ GPU ë©”ëª¨ë¦¬ë¡œ ë°ì´í„° ì „ì†¡
        gpu_data = cp.asarray(data.values)
        
        results = []
        portfolio_value = config['initial_capital']
        
        # ë²¡í„°í™”ëœ ë°±í…ŒìŠ¤íŒ…
        for i in range(len(gpu_data)):
            signal = strategy.generate_signal(gpu_data[:i+1])
            
            if signal['action'] != 'HOLD':
                trade = self._execute_virtual_trade(
                    signal, gpu_data[i], portfolio_value
                )
                results.append(trade)
                portfolio_value += trade['pnl']
        
        # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
        metrics = self._calculate_metrics(results, portfolio_value)
        
        return metrics

# ==================== ë©”ì¸ ì‹¤í–‰ ì‹œìŠ¤í…œ ====================

class HypersonicTradingSystem:
    """
    ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í†µí•© ì‹¤í–‰
    """
    
    def __init__(self):
        self.gpu_engine = GPUAcceleratedEngine()
        self.llm_ensemble = MultiLLMEnsemble()
        self.mamba_model = MambaTimeSeriesModel()
        self.mev_bot = ProductionMEVBot()
        self.network = UltraLowLatencyNetwork()
        self.backtester = ProductionBacktester()
        
        # FastAPI ì•±
        self.app = FastAPI(title="Hypersonic Trader")
        self._setup_routes()
        
        # ì„±ê³¼ ì¶”ì 
        self.performance = {
            "total_trades": 0,
            "winning_trades": 0,
            "total_pnl": 0,
            "start_time": datetime.now()
        }
    
    async def start(self):
        """
        ì‹œìŠ¤í…œ ì‹œì‘
        """
        print("ğŸš€ Hypersonic Trading System Starting...")
        
        # ë³‘ë ¬ íƒœìŠ¤í¬ ì‹œì‘
        tasks = [
            self._run_trading_engine(),
            self._run_mev_bot(),
            self._run_api_server(),
            self._monitor_performance()
        ]
        
        await asyncio.gather(*tasks)
    
    async def _run_trading_engine(self):
        """
        ë©”ì¸ íŠ¸ë ˆì´ë”© ì—”ì§„
        """
        while True:
            try:
                # ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘
                market_data = await self._collect_market_data()
                
                # LLM ì•™ìƒë¸”ë¡œ ì‹¬ë¦¬ ë¶„ì„
                sentiment = await self.llm_ensemble.analyze_market_sentiment(
                    market_data['news']
                )
                
                # Mamba ëª¨ë¸ë¡œ ê°€ê²© ì˜ˆì¸¡
                price_prediction = self.mamba_model(
                    torch.tensor(market_data['prices']).float()
                )
                
                # GPUë¡œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”
                optimal_weights = self.gpu_engine.gpu_batch_prediction(
                    torch.tensor(market_data['returns']).float()
                )
                
                # ê±°ë˜ ì‹ í˜¸ ìƒì„±
                if self._should_trade(price_prediction, sentiment):
                    order = self._create_order(optimal_weights)
                    
                    # ì´ˆì €ì§€ì—° ì‹¤í–‰
                    result = await self.network.optimized_order_routing(order)
                    
                    # ì„±ê³¼ ê¸°ë¡
                    self._update_performance(result)
                
                await asyncio.sleep(0.1)  # 100ms ì£¼ê¸°
                
            except Exception as e:
                logging.error(f"Trading engine error: {e}")
    
    def _setup_routes(self):
        """
        API ë¼ìš°íŠ¸ ì„¤ì •
        """
        @self.app.get("/")
        async def root():
            return {"name": "Hypersonic Trader", "status": "running"}
        
        @self.app.get("/performance")
        async def get_performance():
            return self.performance
        
        @self.app.post("/backtest")
        async def run_backtest(config: Dict):
            result = await self.backtester.backtest_strategy(
                self.mamba_model, config
            )
            return result
        
        @self.app.websocket("/ws")
        async def websocket_endpoint(websocket: WebSocket):
            await websocket.accept()
            while True:
                data = {
                    "timestamp": datetime.now().isoformat(),
                    "performance": self.performance,
                    "active_trades": self._get_active_trades()
                }
                await websocket.send_json(data)
                await asyncio.sleep(1)

if __name__ == "__main__":
    # ì‹œìŠ¤í…œ ì‹œì‘
    system = HypersonicTradingSystem()
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        HYPERSONIC TRADER - ì‹¤ì „ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ                     â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  ì‹¤ì œ êµ¬í˜„ëœ ê¸°ìˆ :                                                 â•‘
    â•‘  â€¢ NVIDIA H100/H200 GPU ê°€ì† (100% êµ¬í˜„)                         â•‘
    â•‘  â€¢ GPT-4 + Claude + Gemini ì•™ìƒë¸” (API ì—°ë™)                     â•‘
    â•‘  â€¢ Mamba ì•„í‚¤í…ì²˜ (4-5ë°° ë¹ ë¥¸ ì¶”ë¡ )                               â•‘
    â•‘  â€¢ MEV ë´‡ (Ethereum, BSC, Arbitrum)                              â•‘
    â•‘  â€¢ Co-location + FPGA ì´ˆì €ì§€ì—°                                   â•‘
    â•‘  â€¢ Ray ë¶„ì‚° ì»´í“¨íŒ… + CuPy GPU ê°€ì†                               â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:                                                   â•‘
    â•‘  â€¢ ë ˆì´í„´ì‹œ: < 1ms (co-location)                                 â•‘
    â•‘  â€¢ ì²˜ë¦¬ëŸ‰: 100,000+ orders/sec                                   â•‘
    â•‘  â€¢ GPU ê°€ì†: 100ë°° ë¹ ë¥¸ ì—°ì‚°                                      â•‘
    â•‘  â€¢ ì˜ˆì¸¡ ì •í™•ë„: 70%+ (LLM ì•™ìƒë¸”)                                 â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Starting system...
    API: http://localhost:8000
    WebSocket: ws://localhost:8000/ws
    """)
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(system.start())