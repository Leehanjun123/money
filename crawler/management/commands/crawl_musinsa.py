from django.core.management.base import BaseCommand
from crawler.musinsa_crawler import MusinsaCrawler
from crawler.models import CrawlLog
from django.utils import timezone
import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    """ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ì‹¤í–‰ ëª…ë ¹ì–´"""
    
    help = 'Run Musinsa crawling for specified categories'
    
    def add_arguments(self, parser):
        parser.add_argument(
            '--categories',
            nargs='+',
            default=['001', '002', '003', '004', '005'],
            help='Categories to crawl (default: ìƒì˜, í•˜ì˜, ì‹ ë°œ, ê°€ë°©, ì•¡ì„¸ì„œë¦¬)'
        )
        parser.add_argument(
            '--max-pages',
            type=int,
            default=50,
            help='Maximum pages to crawl per category'
        )
        parser.add_argument(
            '--delay',
            type=float,
            default=2.0,
            help='Delay between requests in seconds'
        )
        parser.add_argument(
            '--full-crawl',
            action='store_true',
            help='Perform full crawl (all categories, more pages)'
        )
    
    def handle(self, *args, **options):
        start_time = timezone.now()
        
        # í¬ë¡¤ë§ ë¡œê·¸ ì‹œì‘
        crawl_log = CrawlLog.objects.create(
            crawler_name='musinsa_main',
            start_time=start_time,
            status='running'
        )
        
        try:
            crawler = MusinsaCrawler()
            
            # ì „ì²´ í¬ë¡¤ë§ ëª¨ë“œ
            if options['full_crawl']:
                self.stdout.write(
                    self.style.SUCCESS('ğŸš€ ë¬´ì‹ ì‚¬ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ - ì‹œì¥ ë…ì  ëª¨ë“œ!')
                )
                categories = list(crawler.CATEGORIES.keys())
                max_pages = 100
                delay = 1.5
            else:
                categories = options['categories']
                max_pages = options['max_pages']
                delay = options['delay']
            
            self.stdout.write(f'ì¹´í…Œê³ ë¦¬: {categories}')
            self.stdout.write(f'ìµœëŒ€ í˜ì´ì§€: {max_pages}')
            self.stdout.write(f'ìš”ì²­ ê°„ê²©: {delay}ì´ˆ')
            
            total_products = 0
            total_errors = 0
            
            # ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
            for category_id in categories:
                if category_id not in crawler.CATEGORIES:
                    self.stdout.write(
                        self.style.ERROR(f'Unknown category: {category_id}')
                    )
                    continue
                
                category_name = crawler.CATEGORIES[category_id]['name']
                self.stdout.write(
                    self.style.SUCCESS(f'\nğŸ“¦ {category_name} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘...')
                )
                
                try:
                    # ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰
                    result = crawler.crawl_category(
                        category_id,
                        max_pages=max_pages,
                        delay=delay
                    )
                    
                    products_count = result.get('products_count', 0)
                    errors_count = result.get('errors_count', 0)
                    
                    total_products += products_count
                    total_errors += errors_count
                    
                    self.stdout.write(
                        self.style.SUCCESS(
                            f'âœ… {category_name}: {products_count}ê°œ ìƒí’ˆ ìˆ˜ì§‘, {errors_count}ê°œ ì˜¤ë¥˜'
                        )
                    )
                    
                    # ì„œë¸Œì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
                    subcategories = crawler.CATEGORIES[category_id].get('subcategories', {})
                    for sub_id, sub_info in subcategories.items():
                        if isinstance(sub_info, dict) and 'name' in sub_info:
                            sub_name = sub_info['name']
                            
                            self.stdout.write(f'  ğŸ”¸ {sub_name} ì„œë¸Œì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§...')
                            
                            sub_result = crawler.crawl_subcategory(
                                category_id, 
                                sub_id,
                                max_pages=min(max_pages // 2, 25),
                                delay=delay
                            )
                            
                            sub_products = sub_result.get('products_count', 0)
                            sub_errors = sub_result.get('errors_count', 0)
                            
                            total_products += sub_products
                            total_errors += sub_errors
                            
                            self.stdout.write(
                                f'    âœ… {sub_products}ê°œ ìƒí’ˆ, {sub_errors}ê°œ ì˜¤ë¥˜'
                            )
                    
                except Exception as e:
                    logger.error(f"Category {category_id} crawling failed: {e}")
                    self.stdout.write(
                        self.style.ERROR(f'âŒ {category_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}')
                    )
                    total_errors += 1
            
            # í¬ë¡¤ë§ ì™„ë£Œ
            end_time = timezone.now()
            duration = end_time - start_time
            
            crawl_log.end_time = end_time
            crawl_log.status = 'completed'
            crawl_log.products_crawled = total_products
            crawl_log.errors_count = total_errors
            crawl_log.save()
            
            self.stdout.write(
                self.style.SUCCESS(
                    f'\nğŸ‰ ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ!'
                    f'\nğŸ“Š ì´ {total_products}ê°œ ìƒí’ˆ ìˆ˜ì§‘'
                    f'\nâš ï¸ {total_errors}ê°œ ì˜¤ë¥˜ ë°œìƒ'
                    f'\nâ±ï¸ ì†Œìš”ì‹œê°„: {duration}'
                )
            )
            
            # ì‹œì¥ ë…ì  ë©”ì‹œì§€
            if options['full_crawl'] and total_products > 10000:
                self.stdout.write(
                    self.style.SUCCESS(
                        f'\nğŸš€ StyleMate ì‹œì¥ ë…ì  ëª¨ë“œ - {total_products}ê°œ ìƒí’ˆìœ¼ë¡œ ë¬´ì‹ ì‚¬ ì œì••!'
                    )
                )
        
        except Exception as e:
            # í¬ë¡¤ë§ ì‹¤íŒ¨
            crawl_log.end_time = timezone.now()
            crawl_log.status = 'failed'
            crawl_log.error_message = str(e)
            crawl_log.save()
            
            logger.error(f"Crawling failed: {e}")
            self.stdout.write(
                self.style.ERROR(f'âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}')
            )